{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1r02ES23wJaaUEp2hx3SaLkrJzff6rQGe","timestamp":1757514817363}],"gpuType":"T4","authorship_tag":"ABX9TyMFrLqpQcMSTdVmZrYCRoo3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["سهیل حمزه بیگی\n","شماره دانشجویی: ۴۰۳۴۴۳۰۴۷"],"metadata":{"id":"ZO9ZmVgHAsMH"}},{"cell_type":"code","source":["# Runtime -> Change runtime type -> GPU\n","!pip install -q einops"],"metadata":{"id":"2-MNB-MLAsWh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math\n","import os\n","from functools import partial\n","from typing import Tuple\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import transforms, datasets\n","from einops import rearrange\n","import torchvision.transforms.functional as TF\n","from torchvision.transforms import functional as F_tf\n","import torchvision\n","import numpy as np\n","import cv2"],"metadata":{"id":"YzwUhaf0uh_E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pair(x):\n","    return x if isinstance(x, tuple) else (x, x)\n","\n","def exists(val):\n","    return val is not None\n","\n","\n","# Patch Embed\n","class PatchEmbed(nn.Module):\n","    def __init__(self, img_size=32, patch_size=8, in_chans=3, embed_dim=128):\n","        super().__init__()\n","        img_size = pair(img_size)\n","        patch_size = pair(patch_size)\n","        assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0\n","        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n","        self.num_patches = self.grid_size[0] * self.grid_size[1]\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n","        self.embed_dim = embed_dim\n","\n","    def forward(self, x):\n","        # x: (B, C, H, W)\n","        x = self.proj(x)  # (B, E, H/ps, W/ps)\n","        B, E, H, W = x.shape\n","        x = x.flatten(2).transpose(1, 2)  # (B, N, E)\n","        return x, (H, W)"],"metadata":{"id":"I_cCFrueuiNI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transformer components (lightweight)\n","class MLP(nn.Module):\n","    def __init__(self, in_dim, hidden_dim=None, out_dim=None, drop=0.0):\n","        super().__init__()\n","        out_dim = out_dim or in_dim\n","        hidden_dim = hidden_dim or in_dim * 4\n","        self.fc1 = nn.Linear(in_dim, hidden_dim)\n","        self.act = nn.GELU()\n","        self.fc2 = nn.Linear(hidden_dim, out_dim)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        return self.drop(self.fc2(self.act(self.fc1(x))))\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, dim, num_heads=4, mlp_ratio=2.0, drop=0., attn_drop=0.):\n","        super().__init__()\n","        self.norm1 = nn.LayerNorm(dim)\n","        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=attn_drop, batch_first=True)\n","        self.norm2 = nn.LayerNorm(dim)\n","        self.mlp = MLP(dim, int(dim * mlp_ratio), None, drop)\n","\n","    def forward(self, x):\n","        # x: (B, N, C)\n","        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n","        x = x + self.mlp(self.norm2(x))\n","        return x"],"metadata":{"id":"iuK5-2E_uiUQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hierarchical encoder\n","class HierarchicalEncoder(nn.Module):\n","    def __init__(self, img_size=32, scales=(8,4,2), embed_dims=(64,96,128),\n","                 depth_per_scale=(1,2,2), num_heads=(2,3,4), in_chans=3):\n","        super().__init__()\n","        assert len(scales)==len(embed_dims)==len(depth_per_scale)==len(num_heads)\n","        self.scales = scales\n","        self.patch_embeds = nn.ModuleList()\n","        self.pos_embeds = nn.ParameterList()\n","        self.blocks = nn.ModuleList()\n","        for ps, ed, dpt, nh in zip(scales, embed_dims, depth_per_scale, num_heads):\n","            pe = PatchEmbed(img_size=img_size, patch_size=ps, in_chans=in_chans, embed_dim=ed)\n","            self.patch_embeds.append(pe)\n","            num_patches = pe.num_patches\n","            self.pos_embeds.append(nn.Parameter(torch.zeros(1, num_patches, ed)))\n","            # tiny transformer blocks per-scale\n","            b = nn.Sequential(*[TransformerBlock(ed, num_heads=nh, mlp_ratio=2.0) for _ in range(dpt)])\n","            self.blocks.append(b)\n","\n","        # fusion: project concatenated pooled features to fusion_dim\n","        fusion_in = sum(embed_dims)\n","        fusion_out = embed_dims[-1]\n","        self.fusion_proj = nn.Linear(fusion_in, fusion_out)\n","        self.norm = nn.LayerNorm(fusion_out)\n","\n","        # initialize pos_embeds\n","        for p in self.pos_embeds:\n","            nn.init.trunc_normal_(p, std=0.02)\n","\n","    def forward(self, x):\n","        outs = []\n","        grids = []\n","        pooled = []\n","        for pe, pos, blocks in zip(self.patch_embeds, self.pos_embeds, self.blocks):\n","            tokens, grid = pe(x)  # (B, N, C)\n","            tokens = tokens + pos\n","            tokens = blocks(tokens)\n","            outs.append(tokens)\n","            grids.append(grid)\n","            pooled.append(tokens.mean(dim=1))  # global pool of scale\n","        fused = torch.cat(pooled, dim=1)  # (B, sumC)\n","        fused = self.fusion_proj(fused)  # (B, C_out)\n","        fused = self.norm(fused).unsqueeze(1)  # (B, 1, C)\n","        return fused, outs, grids"],"metadata":{"id":"u-n10Rm7vPTG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Decoder\n","class SmallDecoder(nn.Module):\n","    def __init__(self, enc_dim, decoder_dim=128, depth=3, num_heads=4, out_patch_size=8, img_size=32):\n","        super().__init__()\n","        self.in_proj = nn.Linear(enc_dim, decoder_dim)\n","        self.mask_token = nn.Parameter(torch.zeros(1,1,decoder_dim))\n","        self.blocks = nn.ModuleList([TransformerBlock(decoder_dim, num_heads=num_heads) for _ in range(depth)])\n","        self.norm = nn.LayerNorm(decoder_dim)\n","        # predict per patch average RGB (3) for patches of largest scale grid (img_size//out_patch_size)^2 patches\n","        gh = img_size // out_patch_size\n","        self.gh = gh\n","        self.out_dim = 3  # average RGB per patch\n","        self.pred = nn.Linear(decoder_dim, self.out_dim)\n","\n","        nn.init.trunc_normal_(self.mask_token, std=0.02)\n","\n","    def forward(self, fused_token, largest_scale_tokens, mask):\n","        # fused_token\n","        B = fused_token.size(0)\n","        N = largest_scale_tokens.size(1)\n","        dec_in = self.in_proj(fused_token).repeat(1, N, 1)  # (B,N,dec_dim)\n","        if largest_scale_tokens.size(2) != dec_in.size(2):\n","            proj = nn.Linear(largest_scale_tokens.size(2), dec_in.size(2)).to(largest_scale_tokens.device)\n","            vt = proj(largest_scale_tokens)\n","        else:\n","            vt = largest_scale_tokens\n","        # add vt for visible\n","        dec = dec_in.clone()\n","        visible = ~mask\n","        dec[visible] = dec[visible] + vt[visible]\n","        # masked positions: add mask token\n","        dec[mask] = dec[mask] + self.mask_token.repeat(B, mask.size(1), 1)[mask]\n","        x = dec\n","        for blk in self.blocks:\n","            x = blk(x)\n","        x = self.norm(x)\n","        preds = self.pred(x)  # (B, N, 3)\n","        # flatten to B, N*3\n","        return preds.view(B, -1)"],"metadata":{"id":"SixvV7N8vPZr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# MAE\n","class DPHMAE_CIFAR(nn.Module):\n","    def __init__(self, img_size=32, scales=(8,4,2), embed_dims=(64,96,128),\n","                 depth_per_scale=(1,2,2)):\n","        super().__init__()\n","        self.encoder = HierarchicalEncoder(img_size=img_size, scales=scales,\n","                                           embed_dims=embed_dims, depth_per_scale=depth_per_scale)\n","        self.enc_out_dim = embed_dims[-1]\n","        self.decoder = SmallDecoder(enc_dim=self.enc_out_dim, decoder_dim=128, depth=3, num_heads=4,\n","                                    out_patch_size=scales[0], img_size=img_size)\n","        self.scales = scales\n","\n","    def forward(self, images, mask):\n","        fused, outs, grids = self.encoder(images)\n","        # use largest scale outputs for per-patch tokens (outs[0])\n","        largest_tokens = outs[0]  # (B, N, C_small)\n","        preds = self.decoder(fused, largest_tokens, mask)\n","        return preds"],"metadata":{"id":"9szM-5OyvPf6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dynamic Masking + Progressive schedule for CIFAR patch grid\n","class DynamicMaskerCIFAR:\n","    def __init__(self, img_size=32, patch_size=8, base_mask=0.3, target_mask=0.75,\n","                 schedule_epochs=100, mode='variance', device='cpu'):\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.grid = (img_size // patch_size, img_size // patch_size)\n","        self.N = self.grid[0] * self.grid[1]\n","        self.base = base_mask\n","        self.target = target_mask\n","        self.epochs = schedule_epochs\n","        self.mode = mode\n","        self.device = device\n","\n","    def ratio(self, epoch):\n","        t = min(1.0, epoch / max(1, self.epochs))\n","        return self.base + (self.target - self.base) * t  # linear schedule\n","\n","    def compute_importance(self, images):\n","        # images: (B, C, H, W) in [0,1]\n","        B, C, H, W = images.shape\n","        ph = self.patch_size\n","        gh = H // ph\n","        gw = W // ph\n","        patches = images.unfold(2, ph, ph).unfold(3, ph, ph)  # (B, C, gh, gw, ph, ph)\n","        patches = patches.contiguous().view(B, C, gh*gw, ph, ph)\n","        patches = patches.permute(0,2,1,3,4).reshape(B, gh*gw, C*ph*ph)  # (B, N, D)\n","        if self.mode == 'variance':\n","            imp = patches.var(dim=2)  # (B, N)\n","        elif self.mode == 'edge':\n","            # compute Sobel edge magnitude per patch quickly by resizing and cv2.Sobel; but keep pure torch:\n","            gray = images.mean(dim=1, keepdim=True)  # (B,1,H,W)\n","            # simple laplacian via conv\n","            lap = F.conv2d(gray, weight=torch.tensor([[[[0,1,0],[1,-4,1],[0,1,0]]]], dtype=gray.dtype, device=gray.device), padding=1)\n","            p2 = lap.unfold(2, ph, ph).unfold(3, ph, ph).contiguous().view(B, gh*gw, ph*ph)\n","            imp = p2.abs().mean(dim=2)\n","        else:\n","            imp = patches.var(dim=2)\n","        # normalize to probabilities\n","        prob = imp + 1e-6\n","        prob = prob / prob.sum(dim=1, keepdim=True)\n","        return prob  # (B, N)\n","\n","    def sample_mask(self, images, epoch):\n","        B = images.size(0)\n","        r = self.ratio(epoch)\n","        prob = self.compute_importance(images)  # higher -> important\n","        inv = 1.0 - prob\n","        inv = inv / inv.sum(dim=1, keepdim=True)\n","        k = int(self.N * r)\n","        masks = torch.zeros(B, self.N, dtype=torch.bool, device=self.device)\n","        for i in range(B):\n","            if k <= 0:\n","                continue\n","            idx = torch.multinomial(inv[i], k, replacement=False)\n","            masks[i, idx] = True\n","        return masks"],"metadata":{"id":"UhuosMwDx3IR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training on CIFAR-10\n","def build_dataloaders(batch_size=128, img_size=32):\n","    transform = transforms.Compose([\n","        transforms.Resize((img_size, img_size)),\n","        transforms.ToTensor(),\n","    ])\n","    train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","    val_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n","    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n","    return train_loader, val_loader\n","\n","def compute_patch_targets(images, patch_size):\n","    # images\n","    ph = patch_size\n","    pooled = F.avg_pool2d(images, kernel_size=ph, stride=ph)  # (B, C, gh, gw)\n","    B, C, gh, gw = pooled.shape\n","    return pooled.permute(0,2,3,1).reshape(B, gh*gw* C)  # (B, N*3)"],"metadata":{"id":"4MeVRWQdx3QI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_epoch(model, loader, optimizer, masker, epoch, device):\n","    model.train()\n","    total_loss = 0.0\n","    n = 0\n","    for images, _ in loader:\n","        images = images.to(device)\n","        masks = masker.sample_mask(images, epoch)  # (B, N)\n","        preds = model(images, masks)  # (B, N*3)\n","        targets = compute_patch_targets(images, masker.patch_size).to(device)\n","        # compute loss only on masked patches\n","        B = images.size(0)\n","        N = masker.N\n","        preds = preds.view(B, N, 3)\n","        targets = targets.view(B, N, 3)\n","        mask = masks.to(device)\n","        if mask.sum() == 0:\n","            loss = F.mse_loss(preds, targets)\n","        else:\n","            loss = F.mse_loss(preds[mask], targets[mask])\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item() * B\n","        n += B\n","    return total_loss / n"],"metadata":{"id":"k8F_g-A_x3az"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validate_epoch(model, loader, masker, device):\n","    model.eval()\n","    total_loss = 0.0\n","    n = 0\n","    with torch.no_grad():\n","        for images, _ in loader:\n","            images = images.to(device)\n","            # use deterministic mask ratio (e.g., target ratio) for eval or use epoch last\n","            masks = masker.sample_mask(images, masker.epochs)  # evaluate at final difficulty\n","            preds = model(images, masks)\n","            targets = compute_patch_targets(images, masker.patch_size).to(device)\n","            B = images.size(0)\n","            N = masker.N\n","            preds = preds.view(B, N, 3)\n","            targets = targets.view(B, N, 3)\n","            mask = masks.to(device)\n","            if mask.sum() == 0:\n","                loss = F.mse_loss(preds, targets)\n","            else:\n","                loss = F.mse_loss(preds[mask], targets[mask])\n","            total_loss += loss.item() * B\n","            n += B\n","    return total_loss / n"],"metadata":{"id":"6XkG2XD3uij3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train\n","\n","def run_cifar_training(epochs=20, batch_size=128, device=None, save_dir='./checkpoints'):\n","    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(\"Device:\", device)\n","    train_loader, val_loader = build_dataloaders(batch_size=batch_size, img_size=32)\n","    model = DPHMAE_CIFAR(img_size=32, scales=(8,4,2), embed_dims=(64,96,128), depth_per_scale=(1,2,2)).to(device)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05)\n","    masker = DynamicMaskerCIFAR(img_size=32, patch_size=8, base_mask=0.3, target_mask=0.75,\n","                                schedule_epochs=epochs, mode='variance', device=device)\n","    os.makedirs(save_dir, exist_ok=True)\n","    for epoch in range(1, epochs+1):\n","        train_loss = train_epoch(model, train_loader, optimizer, masker, epoch, device)\n","        val_loss = validate_epoch(model, val_loader, masker, device)\n","        print(f\"Epoch {epoch:02d}  Train Loss: {train_loss:.6f}  Val Loss: {val_loss:.6f}  MaskRatio: {masker.ratio(epoch):.3f}\")\n","        torch.save({'model': model.state_dict(), 'opt': optimizer.state_dict(), 'epoch': epoch}, f\"{save_dir}/dph_mae_epoch{epoch}.pth\")\n","    print(\"Training finished.\")\n","\n","# If running interactively in Colab, call:\n","if __name__ == '__main__':\n","    run_cifar_training(epochs=20, batch_size=256)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IPmGwNu8XbTj","outputId":"0735471a-5831-4327-d87a-89a1395be824","executionInfo":{"status":"ok","timestamp":1757596084961,"user_tz":-210,"elapsed":17340,"user":{"displayName":"soheil2000 h","userId":"17327319508455993634"}}},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Device: cuda\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 170M/170M [00:02<00:00, 71.4MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 01  Train Loss: 0.052552  Val Loss: 0.024355  MaskRatio: 0.323\n","Epoch 02  Train Loss: 0.025017  Val Loss: 0.024590  MaskRatio: 0.345\n","Epoch 03  Train Loss: 0.024149  Val Loss: 0.023974  MaskRatio: 0.367\n","Epoch 04  Train Loss: 0.023673  Val Loss: 0.023263  MaskRatio: 0.390\n","Epoch 05  Train Loss: 0.023774  Val Loss: 0.023378  MaskRatio: 0.412\n","Epoch 06  Train Loss: 0.023455  Val Loss: 0.023396  MaskRatio: 0.435\n","Epoch 07  Train Loss: 0.023377  Val Loss: 0.023170  MaskRatio: 0.458\n","Epoch 08  Train Loss: 0.023435  Val Loss: 0.023090  MaskRatio: 0.480\n","Epoch 09  Train Loss: 0.023337  Val Loss: 0.023269  MaskRatio: 0.502\n","Epoch 10  Train Loss: 0.023375  Val Loss: 0.023077  MaskRatio: 0.525\n","Epoch 11  Train Loss: 0.023415  Val Loss: 0.023025  MaskRatio: 0.547\n","Epoch 12  Train Loss: 0.023262  Val Loss: 0.023110  MaskRatio: 0.570\n","Epoch 13  Train Loss: 0.023305  Val Loss: 0.023092  MaskRatio: 0.593\n","Epoch 14  Train Loss: 0.023336  Val Loss: 0.023379  MaskRatio: 0.615\n","Epoch 15  Train Loss: 0.023243  Val Loss: 0.023130  MaskRatio: 0.637\n","Epoch 16  Train Loss: 0.023275  Val Loss: 0.023130  MaskRatio: 0.660\n","Epoch 17  Train Loss: 0.023274  Val Loss: 0.022963  MaskRatio: 0.682\n","Epoch 18  Train Loss: 0.023245  Val Loss: 0.023101  MaskRatio: 0.705\n","Epoch 19  Train Loss: 0.023265  Val Loss: 0.023096  MaskRatio: 0.728\n","Epoch 20  Train Loss: 0.023230  Val Loss: 0.023206  MaskRatio: 0.750\n","Training finished.\n"]}]},{"cell_type":"markdown","source":["## Linear Probing & Fine Tuning"],"metadata":{"id":"FcuAx7ZCX7j2"}},{"cell_type":"code","source":["import torch\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(\"Device:\", device)"],"metadata":{"id":"CpsD32cINuFK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757596084978,"user_tz":-210,"elapsed":0,"user":{"displayName":"soheil2000 h","userId":"17327319508455993634"}},"outputId":"6650e42c-2dd6-4d49-8e7c-48faa200998a"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Device: cuda\n"]}]},{"cell_type":"code","source":["import os, time, math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.datasets import FakeData"],"metadata":{"id":"kMKOI3ivzDYU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_linear_probe(checkpoint_path=\"./checkpoints/dph_mae_epoch20.pth\", epochs=5, batch_size=128):\n","    import torch\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","    data_root = download_imagenette()\n","    tf = transforms.Compose([transforms.Resize((32,32)), transforms.ToTensor()])\n","    train_ds = datasets.ImageFolder(os.path.join(data_root,\"train\"), transform=tf)\n","    val_ds = datasets.ImageFolder(os.path.join(data_root,\"val\"), transform=tf)\n","    train_loader = DataLoader(train_ds,batch_size=batch_size,shuffle=True,num_workers=2)\n","    val_loader = DataLoader(val_ds,batch_size=batch_size,shuffle=False,num_workers=2)\n","\n","    # load encoder (pretrained)\n","    base = DPHMAE_CIFAR()\n","    ckpt = torch.load(checkpoint_path, map_location=device)\n","    base.load_state_dict(ckpt['model'], strict=False)\n","    encoder = base.encoder.to(device)\n","\n","    model = LinearProbe(encoder, enc_dim=128, num_classes=len(train_ds.classes)).to(device)\n","    opt = torch.optim.Adam(model.fc.parameters(), lr=1e-3)\n","\n","    for ep in range(1, epochs+1):\n","        model.train(); correct,total,loss_sum=0,0,0\n","        for imgs, labels in train_loader:\n","            imgs, labels = imgs.to(device), labels.to(device)\n","            logits = model(imgs)\n","            loss = F.cross_entropy(logits, labels)\n","            opt.zero_grad(); loss.backward(); opt.step()\n","            preds = logits.argmax(1)\n","            correct += (preds==labels).sum().item(); total+=labels.size(0)\n","            loss_sum += loss.item()*labels.size(0)\n","        print(f\"[LinearProbe] Epoch {ep} TrainAcc {correct/total:.3f} Loss {loss_sum/total:.4f}\")\n","\n","        # validation\n","        model.eval(); correct,total=0,0\n","        with torch.no_grad():\n","            for imgs, labels in val_loader:\n","                imgs, labels = imgs.to(device), labels.to(device)\n","                preds = model(imgs).argmax(1)\n","                correct += (preds==labels).sum().item(); total+=labels.size(0)\n","        print(f\"[LinearProbe] Epoch {ep} ValAcc {correct/total:.3f}\")\n"],"metadata":{"id":"PCKnet25X72O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Linear probing روی Imagenette\n","run_linear_probe(\"./checkpoints/dph_mae_epoch20.pth\", epochs=3, batch_size=128)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lQ0D6KXX252E","executionInfo":{"status":"ok","timestamp":1757534490884,"user_tz":-210,"elapsed":38012,"user":{"displayName":"soheil2000 h","userId":"17327319508455993634"}},"outputId":"cf806c75-87c7-436f-f71a-e704531615e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[LinearProbe] Epoch 1 TrainAcc 0.175 Loss 2.2368\n","[LinearProbe] Epoch 1 ValAcc 0.220\n","[LinearProbe] Epoch 2 TrainAcc 0.224 Loss 2.1463\n","[LinearProbe] Epoch 2 ValAcc 0.236\n","[LinearProbe] Epoch 3 TrainAcc 0.243 Loss 2.1122\n","[LinearProbe] Epoch 3 ValAcc 0.255\n"]}]},{"cell_type":"code","source":["# UPerNet Fine-tuning (FakeData segmentation)\n","def encode_with_pos_interp(encoder, x):\n","    pooled, outs, grids = [], [], []\n","    device = x.device\n","    for pe, pos, blk in zip(encoder.patch_embeds, encoder.pos_embeds, encoder.blocks):\n","        tokens, grid = pe(x)           # (B, N_new, C)\n","        N_new = tokens.shape[1]\n","        pos_param = pos\n","        N_old = pos_param.shape[1]\n","        if N_old != N_new:\n","            s_old = int(math.sqrt(N_old))\n","            c = pos_param.shape[2]\n","            pos_map = pos_param.reshape(1, s_old, s_old, c).permute(0,3,1,2)\n","            H_new, W_new = grid\n","            pos_map_interp = F.interpolate(pos_map.to(device), size=(H_new, W_new), mode='bilinear', align_corners=False)\n","            pos_interp = pos_map_interp.permute(0,2,3,1).reshape(1, H_new*W_new, c)\n","        else:\n","            pos_interp = pos_param.to(device)\n","        tokens = tokens + pos_interp\n","        tokens = blk(tokens)\n","        pooled.append(tokens.mean(dim=1))\n","        outs.append(tokens)\n","        grids.append(grid)\n","    fused = torch.cat(pooled, dim=1)\n","    fused = encoder.fusion_proj(fused.to(device))\n","    fused = encoder.norm(fused).unsqueeze(1)\n","    return fused, outs, grids\n","\n","\n","# tokens -> feature map\n","def tokens_to_map(tokens):\n","    B,N,C = tokens.shape\n","    s = int(math.sqrt(N))\n","    return tokens.transpose(1,2).reshape(B,C,s,s)"],"metadata":{"id":"lhaSmTr2yzua"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# FPN + PSP head (UPerNet)\n","class SimpleFPN(nn.Module):\n","    def __init__(self,in_channels,out_ch=128):\n","        super().__init__()\n","        self.lats = nn.ModuleList([nn.Conv2d(c,out_ch,1) for c in in_channels])\n","    def forward(self,feats):\n","        target_h, target_w = feats[0].shape[2:]\n","        outs=[]\n","        for f,lat in zip(feats,self.lats):\n","            p = lat(f)\n","            if p.shape[2:] != (target_h,target_w):\n","                p = F.interpolate(p, size=(target_h,target_w), mode='bilinear', align_corners=False)\n","            outs.append(p)\n","        return sum(outs)\n","\n","class PSPModule(nn.Module):\n","    def __init__(self,in_ch,pool_sizes=(1,2,3,6),out_ch=128):\n","        super().__init__()\n","        self.stages = nn.ModuleList([nn.Sequential(\n","            nn.AdaptiveAvgPool2d(s),\n","            nn.Conv2d(in_ch, out_ch, 1),\n","            nn.ReLU(inplace=True)\n","        ) for s in pool_sizes])\n","        self.bottleneck = nn.Conv2d(in_ch + len(pool_sizes)*out_ch, out_ch, 3, padding=1)\n","    def forward(self, x):\n","        h,w = x.shape[2:]\n","        pri = [x]\n","        for st in self.stages:\n","            pri.append(F.interpolate(st(x), size=(h,w), mode='bilinear', align_corners=False))\n","        return self.bottleneck(torch.cat(pri, dim=1))\n","\n","class UPerNetHead(nn.Module):\n","    def __init__(self, in_channels_list, out_ch=128, num_classes=21):\n","        super().__init__()\n","        self.fpn = SimpleFPN(in_channels_list, out_ch=out_ch)\n","        self.psp = PSPModule(out_ch, pool_sizes=(1,2,3,6), out_ch=out_ch)\n","        self.conv_last = nn.Sequential(\n","            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n","            nn.BatchNorm2d(out_ch),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.classifier = nn.Conv2d(out_ch, num_classes, 1)\n","    def forward(self, outs_tokens, target_size):\n","        maps = [ tokens_to_map(t) for t in outs_tokens ]\n","        fused = self.fpn(maps)\n","        psp = self.psp(fused)\n","        x = self.conv_last(psp)\n","        x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n","        return self.classifier(x)"],"metadata":{"id":"FcUiL56T1YtC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# FakeData segmentation dataset\n","def build_fake_seg_loaders(batch_size=8, img_size=64, num_classes=21):\n","    tf_img = transforms.Compose([transforms.Resize((img_size,img_size)), transforms.ToTensor()])\n","    class FakeSeg(torch.utils.data.Dataset):\n","        def __init__(self, n=500, split='train'):\n","            self.ds = FakeData(size=n, image_size=(3,img_size,img_size),\n","                               num_classes=num_classes, transform=tf_img)\n","        def __len__(self): return len(self.ds)\n","        def __getitem__(self, idx):\n","            img, target = self.ds[idx]\n","            # target فقط یک کلاس است → کل تصویر رو همون کلاس می‌کنیم\n","            mask = torch.full((img_size,img_size), target, dtype=torch.long)\n","            return img, mask\n","    tr = DataLoader(FakeSeg(500,'train'), batch_size=batch_size, shuffle=True)\n","    vl = DataLoader(FakeSeg(100,'val'), batch_size=batch_size, shuffle=False)\n","    return tr, vl"],"metadata":{"id":"ncYNbLJb1Y0m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training loop\n","def run_finetune_upernet_fake(checkpoint_path=\"./checkpoints/dph_mae_epoch20.pth\",\n","                              epochs=3, batch_size=8, lr=1e-4, img_size=64):\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    print(\"Device:\", device)\n","\n","    # load pretrained encoder\n","    base = DPHMAE_CIFAR()\n","    ckpt = torch.load(checkpoint_path, map_location='cpu')\n","    state = ckpt.get('model', ckpt)\n","    base.load_state_dict(state, strict=False)\n","    encoder = base.encoder.to(device)\n","\n","    # UPerNet head\n","    head = UPerNetHead(in_channels_list=[64,96,128], out_ch=128, num_classes=21).to(device)\n","\n","    class FullSegModel(nn.Module):\n","        def __init__(self, encoder, head):\n","            super().__init__()\n","            self.encoder = encoder\n","            self.head = head\n","        def forward(self, x):\n","            fused, outs, grids = encode_with_pos_interp(self.encoder, x)\n","            return self.head(outs, target_size=(x.shape[2], x.shape[3]))\n","\n","    model = FullSegModel(encoder, head).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    tr_loader, vl_loader = build_fake_seg_loaders(batch_size=batch_size, img_size=img_size)\n","\n","    for ep in range(1, epochs+1):\n","        model.train()\n","        total_loss=0; n=0\n","        for imgs, masks in tr_loader:\n","            imgs, masks = imgs.to(device), masks.to(device)\n","            logits = model(imgs)\n","            loss = F.cross_entropy(logits, masks)\n","            optimizer.zero_grad(); loss.backward(); optimizer.step()\n","            total_loss += loss.item()*imgs.size(0); n+=imgs.size(0)\n","        print(f\"[Epoch {ep}] Train Loss: {total_loss/n:.4f}\")\n","\n","    torch.save({'model': model.state_dict()}, \"upernet_finetuned_fake.pth\")\n","    print(\"✅ Fine-tuning finished. Model saved as upernet_finetuned_fake.pth\")"],"metadata":{"id":"haHsgE6c1Y7a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fine-tune + UPerNet\n","run_finetune_upernet_fake(\"./checkpoints/dph_mae_epoch20.pth\", epochs=3, batch_size=8, img_size=64)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K8NMbKV63CiM","executionInfo":{"status":"ok","timestamp":1757597207518,"user_tz":-210,"elapsed":12636,"user":{"displayName":"soheil2000 h","userId":"17327319508455993634"}},"outputId":"67bc6663-ced2-456a-a09b-f01f72388c75"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n","[Epoch 1] Train Loss: 3.0836\n","[Epoch 2] Train Loss: 3.0422\n","[Epoch 3] Train Loss: 2.9528\n","✅ Fine-tuning finished. Model saved as upernet_finetuned_fake.pth\n"]}]}]}